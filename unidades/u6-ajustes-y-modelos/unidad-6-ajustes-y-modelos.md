# Unidad 6 - Ajustes y Modelos

Los modelos se utilizan para representar fen√≥menos de la realidad que queremos estudiar/entender. Al realizar una abstracci√≥n del fen√≥meno en un modelo, nos es m√°s simple ver relaciones entre las diferentes partes del mismo que si miramos al fen√≥meno en todo su contexto puesto que muchas veces ese contexto genera ruido y no tiene que ver con el fen√≥meno en s√≠. Algunos ejemplos de modelos pueden ser:

- Modelo de predicci√≥n de selecci√≥n modal de una persona en base a sus atributos.
- Modelo de abandono de la universidad en base a los atributos de la persona y desempe√±o durante el primer semestre
- Predicci√≥n de casos de Covid

![Untitled](./imagenes/Untitled.png)

- Modelo de Gaud√≠ para dise√±ar arcos

Incluso. probablemente, ya hayan estado usando modelos en alguna otra clase de la carrera, como por ejemplo en C√°lculo. El ejemplo de abajo lo copiamos del libro ‚ÄúC√°lculo de una Variable‚Äù de J. Stewart

![Untitled](./imagenes/Untitled%201.png)

![Untitled](./imagenes/Untitled%202.png)

Probablemente, antes de obtener el modelo para T que se muestra arriba, la situaci√≥n era como la imagen de abajo, mediciones de temperatura del aire a diferentes alturas sobre el nivel del suelo. Luego, se realiz√≥ un modelo lineal para predecir la temperatura del aire, variable dependiente, en funci√≥n de la altura con respecto al suelo, variable independiente.

![Untitled](./imagenes/Untitled%203.png)

En este cap√≠tulo vamos a ver uno de los modelos m√°s b√°sicos de estad√≠stica, pero no menos importante, el modelo de regresi√≥n lineal. 

# Regresi√≥n Lineal

Supongamos que tenemos dos variables, x e y, que presentan una relaci√≥n lineal visible entre ellas. Si queremos modelar el fen√≥meno de la imagen de abajo, podr√≠amos usar una recta como vemos en la imagen de abajo.

![Untitled](./imagenes/Untitled%204.png)

Pero c√≥mo sabemos si una recta es mejor que otra? 

## M√©todo de los m√≠nimos cuadrados

Para realizar el ajuste del modelo, en este caso una recta, a los datos vamos a utilizar un m√©todo llamado ‚Äúm√≠nimos cuadrados‚Äù. El objetivo del mismo es encontrar la recta que produzca el menor error general.

![Untitled](./imagenes/Untitled%205.png)

El procedimiento para calcular el error general de una recta es el siguiente:

1. Tomamos una de las rectas candidatas a ser elegidas
2. Calculamos el error que hay entre el valor medido/real y el valor del modelo/estimado, al cu√°l llamamos residuo
3. Elevamos esta diferencia/error al cuadrado
4. Obtenemos el error total sumando los errores cuadrados para cada punto

$$
L = (\hat{y}_1 - (a + b x_1))^2 + (\hat{y}_2 - (a + b x_2))^2 + ... + (\hat{y}_n - (a + b x_n))^2
$$

![Untitled](./imagenes/Untitled%206.png)

Entonces, en nuestro caso, probar√≠amos con diferentes rectas y calcular√≠amos el error general que cada una produce. 

![Untitled](./imagenes/Untitled%207.png)

![Untitled](./imagenes/Untitled%208.png)

![Untitled](./imagenes/Untitled%209.png)

![Untitled](./imagenes/Untitled%2010.png)

En nuestro ejemplo, vemos que a medida que rotamos la recta en sentido horario, el error disminuya hasta un punto y luego vuelve a aumentar. Lo que estamos observando es que el error comienza a disminuir a medida que la pendiente aumenta, hasta cierto punto, donde alcanzamos un error m√≠nimo. A partir de este punto el error vuelve a aumentar.  

![Untitled](./imagenes/Untitled%2011.png)

Esto nos indica que estamos frente a un problema de optimizaci√≥n. Si consigui√©ramos la ecuaci√≥n de la par√°bola que estamos viendo arriba, podr√≠amos buscar el m√≠nimo de la misma usando la primer derivada. Resulta que esa ecuaci√≥n si la conocemos y es la que vimos m√°s arriba:

$$
L = (\hat{y}_1 - (a + b x_1))^2 + (\hat{y}_2 - (a + b x_2))^2 + ... + (\hat{y}_n - (a + b x_n))^2
$$

La cu√°l podemos rescribir como:

$$
L = \sum_{i = 1}^{n}[y_i - (a + bx_i)]^2
$$

Entonces, para obtener los valores de a y b solo debemos realizar las correspondientes derivadas parciales:¬≥

$$
\frac{\partial L}{\partial b} = \sum_{i = 1}^{n}(-2) x_i[y_i - (a + bx_i)] = 0
$$

$$
\frac{\partial L}{\partial a} = \sum_{i = 1}^{n}(-2)[y_i - (a + bx_i)] = 0
$$

Si despejamos la ecuaciones anteriores obtenemos los siguientes valores para a y b:

$$
b = \frac{n\sum_{i = 1}^{n}{x_i y_i} - (\sum_{i = 1}^{n}{x_i})(\sum_{i = 1}^{n}{y_i})}{n(\sum_{i = 1}^{n}{x_i^2})-(\sum_{i = 1}^{n}{x_i})^2}
$$

$$
a = \frac{\sum_{i = 1}^{n}{y_i} - b\sum_{i = 1}^{n}{x_i}}{n} = \bar{y} - b\bar{x}
$$

Donde:

$\bar{y_i}$ = media de los valores de y

$\bar{x_i}$ = media de los valores de x

### Interpretaci√≥n de los coeficientes

El t√©rmino ‚Äúa‚Äù es la ordenada al origen y nos indica el valor de la variable dependiente cuando la independiente es 0

El t√©rmino ‚Äúb‚Äù es la pendiente de la recta y nos indica c√≥mo es la variaci√≥n de la variable dependiente con respecto a la independiente

![Untitled](./imagenes/Untitled%2012.png)

## Ejemplo

Ir a la notebook que preparamos sobre el tema

## Residuos

El residuo es la diferencia entre el valor medido de la variable dependiente, y, el valor estimado por nuestro modelo. La suma de residuos fue el valor que minimizamos para obtener la mejor recta que representaba a nuestros datos. 

$$
Y = \hat{Y} + Residuos
$$

Estudiar los residuos es muy importante para verificar si un modelo lineal es el adecuado para nuestros datos. Cuando graficamos los residuos para los diferentes valores de la variable independiente, x, los mismos deben presentar una distribuci√≥n aleatoria como en la figura de abajo (1) a la izquierda. Sin embargo, si los residuos se distribuyen de alguna forma en particular, como la par√°bola de la imagen de la derecha, entonces el modelo no es el adecuado. En este √∫ltimo caso, lo que sucede es que para valores bajos y altos de la variable independiente, el modelo subestima a la variable dependiente, mientras que para los valores intermedios el modelo sobrestima a la variable dependiente. Al ver el gr√°fico de residuos concluimos que el modelo debe ser revisado.

![Untitled](./imagenes/Untitled%2013.png)

Variables Categ√≥ricas

Las variables categ√≥ricas tambi√©n pueden ser incluidas como variables explicativas en la regresi√≥n lineal. Algunos ejemplos pueden ser:

- G√©nero de la persona (Femenino-Masculino)
- Barrio de la propiedad (Martin - La Sexta - Centro)
- Tiene pileta? (Tiene - No tiene)

Cuando usamos variables categ√≥ricas no se incluyen todas las categor√≠as sino, n-1, puesto que con esa cantidad, ya estamos representando todas las opciones posibles. Por ejemplo:

G√©nero:

- Femenino:
    - 0 - No
    - 1 - S√≠

Barrio:

- Martin:
    - 0 - No
    - 1 - S√≠
- La Sexta:
    - 0 - No
    - 1 - S√≠

Tiene pileta:

- Tiene:
    - 0 - No
    - 1 - S√≠
    
    ![Untitled](./imagenes/Untitled%2014.png)
    

Entonces, las ecuaciones con los diferentes casos se ven de esta forma:

![Untitled](./imagenes/Untitled%2015.png)

# Suavizado y Splines

## Suavizado

El material de esta secci√≥n fue extra√≠do de: [http://rafalab.dfci.harvard.edu/dsbook/smoothing.html](http://rafalab.dfci.harvard.edu/dsbook/smoothing.html)

El suavizado es una t√©cnica muy poderosa utilizada en todo el an√°lisis de datos. Otro nombre dado a esta t√©cnica es ajuste de curva. Est√° dise√±ado para detectar tendencias en presencia de datos ruidosos en casos en los que la forma de la tendencia es desconocida. El nombre de suavizado proviene del hecho de que, para lograr este objetivo, asumimos que la tendencia es suave, como en una superficie suave. En contraste, el ruido o desviaci√≥n de la tendencia es impredeciblemente irregular:

![Untitled](./imagenes/Untitled%2016.png)

Para explicar estos conceptos, nos centraremos primero en un problema con solo un predictor. Espec√≠ficamente, intentamos estimar la tendencia temporal en el margen de la encuesta de voto popular de EE.UU. en 2008 (diferencia entre Obama y McCain).

Para los fines de este ejemplo, no lo consideremos como un problema de pron√≥stico. En cambio, simplemente estamos interesados en aprender la forma de la tendencia despu√©s de que las elecciones hayan terminado.

Suponemos que para cualquier d√≠a dado x, hay una preferencia real entre el electorado $f(x)$, pero debido a la incertidumbre introducida por las encuestas, cada punto de datos viene con un error $\epsilon$. Un modelo matem√°tico para el margen de encuesta observado $Y_i$ es:

$$
Y_i = f(x_i) + \epsilon_i
$$

Consideremos que queremos predecir Y para un d√≠a x. Si supi√©ramos la probabilidad condicional $f(x) = E(Y| X = x)$, la usar√≠amos. Pero dado que no conocemos esta expectativa condicional, tenemos que estimarla. Utilicemos regresi√≥n, ya que es el m√©todo que hemos aprendido hasta ahora.

![Untitled](./imagenes/Untitled%2017.png)

La l√≠nea que vemos no parece describir muy bien la tendencia. Por ejemplo, el 4 de septiembre (d√≠a -62) se llev√≥ a cabo la Convenci√≥n Republicana y los datos sugieren que le dio un impulso a John McCain en las encuestas. Sin embargo, la l√≠nea de regresi√≥n no captura esta posible tendencia. Para ver la falta de ajuste con mayor claridad, observamos que los puntos por encima de la l√≠nea ajustada (azul) y los puntos por debajo (rojo) no est√°n distribuidos de manera uniforme a lo largo de los d√≠as. Por lo tanto, necesitamos un enfoque alternativo y m√°s flexible.

### Suavizado por segmentos

La idea general del suavizado es agrupar los puntos de datos en estratos en los cuales se puede suponer que el valor de $f(x)$ es constante. Podemos hacer esta suposici√≥n porque pensamos que  $f(x)$ cambia lentamente y, como resultado, $f(x)$ es casi constante en peque√±as ventanas de tiempo. Un ejemplo de esta idea para los datos de encuesta_2008 es asumir que la opini√≥n p√∫blica se mantuvo aproximadamente igual durante una semana. Con esta suposici√≥n en su lugar, tenemos varios puntos de datos con el mismo valor esperado.

Si fijamos un d√≠a para que est√© en el centro de nuestra semana, llam√©moslo $x_0$, entonces para cualquier otro d√≠a $x$ tal que, asumimos que $f(x)$ es una constante $f(x) = \mu$. Esta suposici√≥n implica que:

$$
E[Y_i|X_i = x_i] \approx \mu if |x_i - x_0| \le 3.5
$$

En el suavizado, llamamos al tama√±o del intervalo que satisface $|x_i - x_0| \le 3.5$ el tama√±o de la ventana, ancho de banda o rango. M√°s adelante veremos que intentamos optimizar este par√°metro.

Esta suposici√≥n implica que una buena estimaci√≥n para $f(x)$ es el promedio de los valores $Y_i$ en la ventana. Si definimos $A_0$ como el conjunto de √≠ndices $i$ tal que $|x_i - x_0| \le 3.5$ y $N_0$ como el n√∫mero de √≠ndices en $A_0$, entonces nuestra estimaci√≥n es:

$$
\hat{f}(x_0) = \frac{1}{N_0} \sum_{i\in A_0}{}{Y_i}
$$

La idea detr√°s del suavizado por segmento es realizar este c√°lculo con cada valor de $x$ como centro. En el ejemplo de la encuesta, para cada d√≠a, calcular√≠amos el promedio de los valores dentro de una semana con ese d√≠a en el centro. Aqu√≠ hay dos ejemplos: $x_0 = -125$ y $x_0 = -55$. El segmento azul representa el promedio resultante.

![Untitled](./imagenes/Untitled%2018.png)

Al calcular esta media para cada punto, formamos una estimaci√≥n de la curva subyacente $f(x)$. A continuaci√≥n, mostramos el procedimiento a medida que nos desplazamos desde -155 hasta 0. En cada valor de $x_0$, mantenemos la estimaci√≥n y pasamos al siguiente punto, el resultado final es el siguiente

![Untitled](./imagenes/Untitled%2019.png)

### Kernels

El resultado final del suavizado por segmento es bastante irregular. Una raz√≥n de esto es que cada vez que la ventana se mueve, dos puntos cambian. Podemos atenuar esto en cierta medida tomando promedios ponderados que otorgan m√°s peso al punto central que a los puntos lejanos, y los dos puntos en los extremos reciben muy poco peso.

Podemos pensar en el enfoque del suavizado por segmentos como un promedio ponderado:

$$
\hat{f}(x_0) = \sum_{i=1}^{N}{w_0 (x_i)Y_i}
$$

en el cual cada punto recibe un peso de 0 o $1/N_0$, siendo $N_0$ el n√∫mero de puntos en la semana. Las im√°genes de abajo muestran dos maneras de ponderar los datos, a la izquierda vemos una forma uniforme o sin ponderaci√≥n y a la derecha una forma normal.

Si usamos la ponderaci√≥n normal obtenemos una imagen como la de abajo:

![Untitled](./imagenes/Untitled%2020.png)

A continuaci√≥n un ejemplo de suavizado por Kernel, usando la forma Box y Normal:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KernelDensity

# Creamos algunos datos
np.random.seed(0)
X = np.concatenate((np.random.normal(0, 1, 100), np.random.normal(5, 1, 100)))[:, np.newaxis]

# Creamos los modelos de suavizado por kernel
kde_gauss = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(X)
kde_box = KernelDensity(kernel='tophat', bandwidth=0.75).fit(X)  # 'tophat' es un kernel box

# Creamos una serie de puntos donde evaluaremos las densidades
X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]

# Calculamos las densidades
dens_gauss = np.exp(kde_gauss.score_samples(X_plot))  # Usamos exp porque score_samples devuelve el logaritmo de la densidad
dens_box = np.exp(kde_box.score_samples(X_plot))

# Graficamos los resultados
plt.figure(figsize=(10, 6))
plt.plot(X_plot[:, 0], dens_gauss, label="Kernel Gaussiano", color='blue')
plt.plot(X_plot[:, 0], dens_box, label="Kernel Box", color='red')
plt.hist(X, bins=30, density=True, alpha=0.5)
plt.legend()
plt.title("Comparaci√≥n de suavizado por kernel: Gaussiano vs. Box")
plt.show()
```

En este ejemplo, generamos dos conjuntos de datos a partir de distribuciones normales con medias 0 y 5. Luego creamos dos modelos de s44uavizado por kernel, uno con un kernel Gaussiano y otro con un kernel box, y calculamos las densidades en un conjunto de puntos que abarcan el rango de los datos. Por √∫ltimo, graficamos las densidades calculadas junto con un histograma de los datos.

![Untitled](./imagenes/Untitled%2021.png)

### Regresi√≥n Local Ponderada (loess)

Una limitaci√≥n del enfoque del suavizado por bin descrito anteriormente es que necesitamos ventanas peque√±as para que las suposiciones de aproximada constancia se cumplan. Como resultado, terminamos con un peque√±o n√∫mero de puntos de datos para promediar y obtener estimaciones imprecisas $\hat{f}(x)$. Aqu√≠ describimos c√≥mo la regresi√≥n ponderada local (loess) nos permite considerar tama√±os de ventana m√°s grandes. Para hacer esto, utilizaremos un resultado matem√°tico conocido como el teorema de Taylor, que nos dice que si examinas lo suficientemente de cerca cualquier funci√≥n suave $f(x)$, se ver√° como una l√≠nea. 

<aside>
üí° LOESS (LOcally Estimated Scatterplot Smoothing) y LOWESS (LOcally WEighted Scatterplot Smoothing) son esencialmente lo mismo. Ambos son m√©todos de suavizado que utilizan regresiones lineales ponderadas localmente para suavizar datos. La √∫nica diferencia entre los dos t√©rminos es m√°s bien hist√≥rica y proviene de diferentes comunidades de investigaci√≥n. LOESS se utiliza a menudo en el campo de la estad√≠stica, mientras que LOWESS se utiliza a menudo en el campo de la inform√°tica.

</aside>

En lugar de asumir que la funci√≥n es aproximadamente constante en una ventana, asumimos que la funci√≥n es localmente lineal. Podemos considerar tama√±os de ventana m√°s grandes con la suposici√≥n lineal que con una constante. En lugar de la ventana de una semana, consideramos una m√°s grande en la cual la tendencia es aproximadamente lineal. Comenzamos con una ventana de tres semanas y luego consideramos y evaluamos otras opciones.

$$
E[Y_i| X_i = x_i] = \beta_0 + \beta_1 (x_i - x_0) \;\; if \;\; |x_i - x_0| \le 21
$$

Para cada punto $/x_0$, loess define una ventana y ajusta una l√≠nea dentro de esa ventana. Aqu√≠ hay un ejemplo que muestra los ajustes para x0 = -125 y x0 = -55:

![Untitled](./imagenes/Untitled%2022.png)

El valor estimado para $x_0$ se convierte en nuestra estimaci√≥n $\hat{f}(x_0)$. Abajo se muestran los resultados para diferentes spans:

![Untitled](./imagenes/Untitled%2023.png)

<aside>
üí° La principal ventaja de LOESS sobre el suavizado por segmentos es su flexibilidad. Puede manejar relaciones m√°s complejas y no lineales entre las variables, mientras que el suavizado por segmentos puede estar limitado en este aspecto, especialmente si la elecci√≥n de los puntos de quiebre o segmentos no es adecuada. Adem√°s, LOESS tiende a ser menos susceptible al sobreajuste que el suavizado por segmentos, especialmente si se elige un valor adecuado para el par√°metro de suavizado.

</aside>

A continuaci√≥n se muestra un ejemplo de suavizado con LOESS, usando **`statsmodels`** y utilizando el conjunto de datos de iris incorporado en **`seaborn`**:

```python
from statsmodels.nonparametric.smoothers_lowess import lowess
import matplotlib.pyplot as plt
import seaborn as sns

# Cargamos el conjunto de datos de iris
iris = sns.load_dataset('iris')

# Establecemos nuestros valores x e y
x = iris['sepal_length']
y = iris['petal_length']

# Realizamos el ajuste de la regresi√≥n LOWESS con diferentes valores para frac
smoothed_1 = lowess(y, x, frac=0.2)  # 20% de los datos para el ajuste
smoothed_2 = lowess(y, x, frac=0.5)  # 50% de los datos para el ajuste

# Creamos la gr√°fica
plt.figure(figsize=(10, 6))
plt.scatter(x, y, label='Datos originales', color='blue')
plt.plot(smoothed_1[:, 0], smoothed_1[:, 1], color='red', label='LOWESS 0.2')
plt.plot(smoothed_2[:, 0], smoothed_2[:, 1], color='green', label='LOWESS 0.5')
plt.legend()
plt.xlabel('sepal_length')
plt.ylabel('petal_length')
plt.title('Ejemplo de ajuste de LOWESS con diferentes factores de suavizado en Python')
plt.show()
```

El resultado es el siguiente:

![Untitled](./imagenes/Untitled%2024.png)

### Par√°bolas de ajuste

El teorema de Taylor tambi√©n nos dice que si observas una funci√≥n matem√°tica lo suficientemente de cerca, se parece a una par√°bola. El teorema tambi√©n establece que no es necesario observar tan de cerca al aproximar con par√°bolas como cuando se aproxima con l√≠neas. Esto significa que podemos hacer nuestras ventanas a√∫n m√°s grandes y ajustar par√°bolas en lugar de l√≠neas.

$$
E[Y_i|X_i = x_i] = \beta_0 + \beta_1 (x_i - x_0) + \beta_2 (x_i - x_0)^2 \;\; if \;\; |x_i - x_0| \le h
$$

La imagen de abajo muestra una comparaci√≥n de las l√≠neas de ajuste (rojo discontinuo) y las par√°bolas de ajuste (naranja s√≥lido):

![Untitled](./imagenes/Untitled%2025.png)

## Splines polin√≥micas

Una spline polin√≥mica es una funci√≥n matem√°tica que est√° definida de a segmentos. Esto √∫ltimo quiere decir que para cada intervalo de datos vamos a tener una funci√≥n polin√≥mica de grado k que los represente. Esta funci√≥n es continua y suave, es decir, tiene derivadas continuas en los √≥rdenes k-1. Si bien pueden generarse splines de diferentes grados, la m√°s utilizada es la c√∫bica. 

Las splines se utilizan en interpolaci√≥n, splines de interpolaci√≥n, ya que nos permiten obtener una funci√≥n que pase por todos los puntos de inter√©s. En la imagen de abajo, [link](https://www.youtube.com/watch?v=queK1reC-ac&ab_channel=BrianZaharatos), vemos en l√≠nea de trazos una spline que puede servirnos para interpolaci√≥n. Sin embargo, esta funci√≥n, probablemente, no ser√° muy √∫til para modelar nuestros datos puesto que claramente esta haciendo un overfitting de los mismos. Para el modelado de los datos se utiliza una spline suave, dibujado con un trazo lleno abajo.

![Untitled](./imagenes/Untitled%2026.png)

Para obtener la spline suave vamos a minimizar la siguiente funci√≥n:

$$
L = \sum_{i = 1}^{n}[y_i - f(x)]^2 + \lambda \int [f''(x)]^2dx 
$$

La primera parte de la funci√≥n son los m√≠nimos cuadrados que utilizamos para estimar los par√°metros de la regresi√≥n lineal. La segunda parte de la funci√≥n, es un t√©rmino de regulaci√≥n o penalizaci√≥n. Nosotros queremos tener una funci√≥n $f(x)$ que tenga cierta curvatura pero no queremos que la misma sea demasiado grande para que no copie los datos y produzca un overfit. Entonces, incorporamos este t√©rmino al loss/costo, funciones con mayor curvatura van a generar m√°s loss/costo. El par√°metro $\lambda$ regula la importancia del t√©rmino de penalizaci√≥n:

- $\lambda = 0$, implica que no vamos a realizar una penalizaci√≥n por curvatura y por lo tanto el resultado que obtenemos es una spline de interpolaci√≥n
- $\lambda > 0$, implica que vamos a considerar cierta penalizaci√≥n sobre las funciones con mayor curvatura
- $\lambda = \infty$, implica que la penalizaci√≥n es lo m√°s importante y por lo tanto tendremos una recta horizontal

Para poder estimar las splines en python podemos usar el paquete scipy, [el ejemplo de su documentaci√≥n](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html)

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import UnivariateSpline

# Generamos los datos de prueba, agregando ruido aleatorio
rng = np.random.default_rng()
x = np.linspace(-3, 3, 50)
y = np.exp(-x**2) + 0.1 * rng.standard_normal(50)
plt.plot(x, y, 'ro', ms=5)

spl = UnivariateSpline(x, y) # s = len data, the default
xs = np.linspace(-3, 3, 1000)
plt.plot(xs, spl(xs), 'g', lw=3)

spl.set_smoothing_factor(2)
plt.plot(xs, spl(xs), 'pink', lw=3)

spl.set_smoothing_factor(0.5)
plt.plot(xs, spl(xs), 'orange', lw=3)

spl.set_smoothing_factor(0)
plt.plot(xs, spl(xs), 'b', lw=3)
plt.show()
```

Cuando usamos un $\lambda$, tambi√©n llamado t√©rmino de suavizado, igual a 0, obtenemos la spline de interpolaci√≥n, en azul. A medida que vamos incrementando el valor de $\lambda$ obtenemos una funci√≥n con cada vez menos curvatura.

![Untitled](./imagenes/Untitled%2027.png)

Esto que vimos arriba se llama regresi√≥n **no param√©trica** puesto que nosotros no sabemos cu√°l va a ser la funci√≥n antes de la estimaci√≥n, en contraste con la regresi√≥n lineal, param√©trica, donde sab√≠amos la forma de nuestra funci√≥n y los par√°metros que √≠bamos a estimar. 

## Evaluaci√≥n del modelo

La regresi√≥n es importante y conocer las m√©tricas que se utilizan nos ayudan a entender su comportamiento y saber que tan correcta es. Lamentablemente mucha gente s√≥lo usa un par de m√©tricas, muchas veces debido al desconocimiento de todas las que tenemos disponibles. Ahora veremos diferentes m√©tricas en regresi√≥n para que podamos usarlas en nuestros proyectos.

El error es un concepto muy sencillo, simplemente es la diferencia que existe entre el valor que nuestro modelo ha predicho y el valor real de la observaci√≥n con la que estamos haciendo el test.

![Untitled](./imagenes/Untitled%2028.png)

### $R^2$  **(R cuadrado)**

Existen diferentes m√©tricas para conocer el error. Veremos las m√°s importantes y cuando debemos de usarlas.

El $R^2$  o coeficiente de determinaci√≥n se define como: 

$$
R^2 = 1 - \frac{SSR}{SST}
$$

Donde:

$SSR$: es la suma de los residuos cuadrados. Es decir, sumamos los cuadrados de la diferencia entre el valor real y estimado de la variable

$$
SSR = \sum_{i=1}^{n}{(yi - \hat{yi})^2}
$$

$SST$: se denomina suma de los cuadrados totales. Se calcula como la suma de los cuadrados de la diferencia entre el valor real de la variable y el valor medio de la misma.

$$
SST = \sum_{i = 1}^{n}{(yi - \bar{y})^2}
$$

El valor de $R^2$  var√≠a entre 0 y 1. Cuando es 0 significa que el modelo no est√° explicando nada de la variaci√≥n de la variable $y$. Cuando es 1, significa que los datos se encuentran todos sobre la recta del modelo, algo que no sucede en la vida real. 

Este coeficiente explica qu√© porcentaje de la variaci√≥n de la variable dependiente $y$  es explicado por el modelo. 

Para visualizar mejor como se obtiene el $R^2$ , veamos la siguiente gr√°fica:

![Untitled](./imagenes/Untitled%2029.png)

Tambi√©n les recomendamos mirar este [video](https://www.youtube.com/watch?v=2AQKmw14mHM&ab_channel=StatQuestwithJoshStarmer) que puede ayudar a interpretar la f√≥rmula de $R^2$

### $R^2 - ajustado$

Cuando tenemos m√°s de una variable independiente es recomendable utilizar el $R^2 - ajustado$. La raz√≥n para esto √∫ltimo se debe a que a medida que incorporamos m√°s variables explicativas, el $R^2$ va a aumentar, aunque sea de modo marginal, y puede confundir nuestra percepci√≥n. Es decir, puede parecernos que el modelo tiene un buen ajuste, pero este no se debe a que se seleccionaron buenas variables sino, a que se incorporaron muchas variables. 

$$
R^2-ajustado = 1-[\frac{(1-R^2)(n-1)}{n-p-1}]
$$

Donde:

$R^2$: coeficiente de determinaci√≥n

$n$: n√∫mero de datos en la muestra

$p$: cantidad de variables independientes

En la f√≥rmula de arriba podemos observar que cuando aumentamos el n√∫mero de variables explicativas en nuestro modelo, m√°s se penalizar√° el valor del $R^2$. 

### Valor-p (p-value) e Intervalo de confianza

El valor p nos sirve para determinar si una variable es estad√≠sticamente significativa o no. Cuando estimamos el coeficiente de una variable en realidad estamos estimando el valor esperado del mismo, osea, ese coeficiente tiene una distribuci√≥n normal con su media y su varianza. No vamos a entrar en detalles sobre porqu√© sucede esto porque est√° fuera del alcance de nuestro curso. Sin embargo, la siguiente imagen puede darnos una intuici√≥n sobre lo que sucede. Los datos que nosotros obtenemos para realizar nuestro modelo son una muestra de la realidad. Por ejemplo, si tenemos un dato de precio de alquiler donde un departamento de 60$[m^2]$ cuesta 100.000 $[\$]$ eso no quiere decir que todos los departamentos de 60 $[m^2]$ van a costar lo mismo. Probablemente, haya otros departamentos con la misma superficie que cuesten m√°s o menos, y tal vez, en nuestra muestra no tenemos esos datos recolectados. Por esa raz√≥n, cuando estimamos nuestro modelo, en lugar de estimar un valor para cada coeficiente estimamos su media y alguna informaci√≥n sobre su distribuci√≥n.

image from: An Introduction to Mathematical Statistics and Its Applications. Larsen, R. and Marx, M. 2012

![Untitled](./imagenes/Untitled%2030.png)

Esta situaci√≥n nos permite hacer inferencias sobre los coeficientes que estamos estimando, es decir, podemos calcular un intervalo de confianza para los mismos. Si dentro de nuestro intervalo encontramos al valor 0, entonces, puede ser que nuestro coeficiente sea 0, y por lo tanto, no sea de gran valor para explicar a nuestra variable dependiente y. 

Por otro lado, el valor p indica la probabilidad de haber obtenido un coeficiente diferente de cero para cierta variable independiente, cuando en efecto, el coeficiente deb√≠a ser cero, y por lo tanto, la variable en cuesti√≥n, no es un buen predictor para nuestra variable dependiente. Si bien es un poco m√°s dif√≠cil de entender, este valor es m√°s r√°pido de evaluar.

En el ejemplo de abajo vemos que la probabilidad de que el coeficiente para la variable `fem_dummy` sea igual a 0 es 3.2%.

![Untitled](./imagenes/Untitled%2031.png)

Los valores p en un modelo de regresi√≥n lineal tienen una importancia significativa, ya que ofrecen una forma de evaluar si las variables independientes en el modelo tienen un efecto estad√≠sticamente significativo en la variable dependiente. Los valores p son importantes principalmente por los siguientes motivos:

**Prueba de Hip√≥tesis:**

Los valores p est√°n relacionados con la [prueba de hip√≥tesis en la estad√≠stica](https://es.wikipedia.org/wiki/Contraste_de_hip%C3%B3tesis). Para cada coeficiente en el modelo de regresi√≥n, se realiza una prueba de hip√≥tesis para evaluar si el coeficiente es diferente de cero.

- **Hip√≥tesis Nula**: El coeficiente es igual a cero, lo que significa que la variable independiente correspondiente no tiene ning√∫n efecto en la variable dependiente.
- **Hip√≥tesis Alternativa**: El coeficiente es diferente de cero, lo que significa que la variable independiente correspondiente tiene un efecto en la variable dependiente.

**Significancia Estad√≠stica:**

Un valor p es una medida de evidencia en contra de la hip√≥tesis nula. Un valor p peque√±o (por lo general, menor que 0.05) indica que hay evidencia fuerte en contra de la hip√≥tesis nula, y por lo tanto, se rechaza.

- Si el valor p es peque√±o, concluimos que la variable independiente correspondiente tiene un efecto significativo en la variable dependiente.
- Si el valor p es grande, no hay suficiente evidencia para rechazar la hip√≥tesis nula, y no podemos concluir que la variable independiente tenga un efecto en la variable dependiente.

**Selecci√≥n de Variables:**

Los valores p pueden ser √∫tiles en la selecci√≥n de variables en un modelo de regresi√≥n. Las variables con valores p bajos son m√°s propensas a ser importantes en la predicci√≥n de la variable dependiente. Eliminar variables con valores p altos puede llevar a un modelo m√°s simple y m√°s interpretable sin sacrificar mucho en t√©rminos de ajuste.

### **Error absoluto medio, mean absolute error (MAE)**

Esta m√©trica es una medida de la diferencia entre dos valores, es decir, nos permite saber que tan diferente es el valor predicho y el valor real u observado. Para que un error con valor positivo no cancele a un error con error negativo usamos el valor absoluto de la diferencia. Como nos interesa conocer el comportamiento del error de todas las observaciones y no solamente de una, entonces obtenemos el promedio de los valores absolutos de la diferencia.

$$
MAE = \frac{1}{n}\sum_{i = 1}^{n}|y_i - \hat{y_i}|
$$

Este coeficiente mide la media de la distancia absoluta entre los valores reales y estimados de la variable dependiente $y$

Supongamos que tenemos las siguientes observaciones, representan a los valores reales u observados.

Reales=[3, 2.5, -2, 2.3, 4, 7.2, 8.1]

Y los valores a continuaci√≥n representan los que nos da nuestro modelo.

Modelo=[3.1, 2.5, -2.5, 2.45, 3.8, 7.8, 7.9]

Ahora calculamos el MAE y el valor que obtenemos es de¬†**0.249**, es decir que de todos los errores en las observaciones, el error promedio es de¬†**0.249**

### **Error medio cuadrado, mean square error (MSE)**

Esta m√©trica es muy √∫til para saber que tan cerca es la l√≠nea de ajuste de nuestra regresi√≥n a las observaciones. Al igual que en caso anterior evitamos que un error con valor positivo anule a uno con valor negativo, pero en lugar de usar el valor absoluto, elevamos al cuadrado la diferencia.

Siempre nos da un valor positivo y entre m√°s cercano sea a cero es mejor. Se define como:

$$
MSE = \frac{1}{n}\sum_{i = 1}^{n}(y_i - \hat{y_i})^2
$$

<aside>
üí° MAE es m√°s robusto cuando los datos tienen outliers o datos at√≠picos y es la mejor opci√≥n a usar en esos casos. Otra ventaja del MAE es que su escala es la misma que la de los datos de destino, lo que hace que sea f√°cil de interpretar.

</aside>

Este coeficiente mide la media de la distancia cuadrada entre los valores reales y estimados de la variable dependiente $y$. 

<aside>
üí° El MSE da m√°s peso a los errores grandes, es decir, penaliza m√°s los errores grandes.

</aside>

En nuestro ejemplo anterior, al calcular el MSE obtenemos¬†**0.103**, algo que no tenemos que olvidar es que el valor que obtenemos esta en¬†**unidades cuadradas**.

Los coeficientes MAE y MSE dan una medida de la performance del modelo. Valores m√°s altos indican que los datos se encuentran m√°s alejados del modelo, y por lo tanto, este √∫ltimo, puede no ser una buena representaci√≥n para los mismos. 

## Regresi√≥n lineal m√∫ltiple

La regresi√≥n lineal m√∫ltiple es un m√©todo estad√≠stico que permite examinar la relaci√≥n entre dos o m√°s variables explicativas y una variable respuesta. Es una extensi√≥n del modelo de regresi√≥n lineal simple, que s√≥lo permite una variable explicativa.

El modelo de regresi√≥n lineal m√∫ltiple se expresa de la siguiente forma:

$$
y = a + b_1 x_1 + b_2x_2 + ...+b_nx_n + \epsilon
$$

En esta ecuaci√≥n vamos a estimar una constante y un par√°metro para cada una de las n variables explicativas.

Donde:

- Y es la variable respuesta (la que queremos predecir o explicar)
- X1, X2, ..., Xn son las variables explicativas o independientes
- b0, b1, ..., bn son los coeficientes de regresi√≥n, que se estiman a partir de los datos. Estos indican el efecto que tiene cada variable independiente en la variable dependiente.
- e es el error aleatorio

Los coeficientes de regresi√≥n (b0, b1, ..., bn) se interpretan de la siguiente manera:

- b0 es la ordenada en el origen, es decir, el valor esperado de Y cuando todas las variables explicativas son cero.
- b1 es la variaci√≥n en Y asociada a una variaci√≥n de una unidad en X1, manteniendo constantes las dem√°s variables explicativas.
- En general, cada coeficiente bi (i>0) es la variaci√≥n en Y asociada a una variaci√≥n de una unidad en Xi, manteniendo constantes las dem√°s variables.

Los coeficientes se suelen estimar a trav√©s del m√©todo de m√≠nimos cuadrados ordinarios (MCO), que minimiza la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos de la variable dependiente.

## Ejemplo

El primer ejemplo que les aconsejamos revisar es el visto en clase, que se encuentra en la notebook, donde predecimos los costos de obra social.

Otro ejemplo en Python, usando stats model, podr√≠a ser el siguiente, una predicci√≥n de precios de casas a partir de un dataset con m√∫ltiples caracter√≠sticas de viviendas:

- Avg. Area Income: es el ingreso promedio de las personas que viven en esta √°rea.
- Avg. Area House Age: es la edad promedio de las casas ubicadas en el √°rea.
- Avg. Area Number of Rooms: es el n√∫mero promedio de habitaciones de las casas en esta √°rea.
- Avg. Area Number of Bedrooms: es el n√∫mero promedio de dormitorios de las casas en esta √°rea.
- Area Population: Poblaci√≥n del √°rea.
- Address: es la direcci√≥n de la casa.

En nuestro ejemplo, estamos usando esas variables (salvo Address) para entrenar el modelo.

```python
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split

df = pd.read_csv('https://raw.githubusercontent.com/connectaditya/House-price-prediction/master/USA_Housing.csv')

# Definir las variables independientes y la dependiente
X = df[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms', 
        'Avg. Area Number of Bedrooms', 'Area Population']]
Y = df['Price']

# Agregar una constante a las variables independientes (intercepto)
X = sm.add_constant(X)

# Crear y entrenar el modelo usando statsmodels
model = sm.OLS(Y, X).fit()

# Imprimir el resumen del modelo, incluyendo coeficientes y valores-p
print(model.summary())
```

Este c√≥digo utiliza `summary()`, que nos dar√° un resumen completo del modelo de regresi√≥n, incluyendo estad√≠sticas como los coeficientes, los valores t, los valores p, el R¬≤, etc. 

```
OLS Regression Results                            
==============================================================================
Dep. Variable:                  Price   R-squared:                       0.919
Model:                            OLS   Adj. R-squared:                  0.919
Method:                 Least Squares   F-statistic:                     9044.
Date:                Sat, 12 Aug 2023   Prob (F-statistic):               0.00
Time:                        23:16:18   Log-Likelihood:                -51755.
No. Observations:                4000   AIC:                         1.035e+05
Df Residuals:                    3994   BIC:                         1.036e+05
Df Model:                           5                                         
Covariance Type:            nonrobust                                         
================================================================================================
                                   coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------------------------
const                        -2.647e+06   1.91e+04   -138.228      0.000   -2.68e+06   -2.61e+06
Avg. Area Income                21.6604      0.149    144.946      0.000      21.367      21.953
Avg. Area House Age           1.658e+05   1598.673    103.717      0.000    1.63e+05    1.69e+05
Avg. Area Number of Rooms     1.203e+05   1779.180     67.632      0.000    1.17e+05    1.24e+05
Avg. Area Number of Bedrooms  2193.0956   1461.592      1.500      0.134    -672.440    5058.631
Area Population                 15.2859      0.161     94.837      0.000      14.970      15.602
==============================================================================
Omnibus:                        4.735   Durbin-Watson:                   2.016
Prob(Omnibus):                  0.094   Jarque-Bera (JB):                4.353
Skew:                          -0.034   Prob(JB):                        0.113
Kurtosis:                       2.854   Cond. No.                     9.42e+05
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 9.42e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
```

Un valor de R¬≤ m√°s cercano a 1 indica que el modelo es capaz de explicar una gran proporci√≥n de la variaci√≥n en la variable dependiente. Un valor de R¬≤ m√°s cercano a 0, por otro lado, indica que el modelo no puede explicar mucha variaci√≥n en la variable dependiente. En nuestro caso, hemos obtenido un modelo con R¬≤ igual a 0.91 üòâ

<aside>
üí° Los valores p en particular nos ayudar√°n a determinar si cada variable independiente es estad√≠sticamente significativa en la predicci√≥n de la variable dependiente. Un valor p menor que un nivel de significancia t√≠pico (como 0.05) sugiere que hay evidencia fuerte contra la hip√≥tesis nula, lo que indica que la caracter√≠stica es relevante para el modelo.

</aside>

### Representaci√≥n gr√°fica

En regresiones lineales m√∫ltiples, se vuelve complicado visualizar el modelo en un gr√°fico debido a que tenemos m√°s de dos dimensiones (cada variable independiente a√±ade una dimensi√≥n). En cambio, podr√≠amos visualizar la relaci√≥n entre la variable dependiente y cada una de las variables independientes individualmente.

Sin embargo, para visualizar la precisi√≥n del modelo, se puede usar un gr√°fico de dispersi√≥n que compara los valores reales de la variable dependiente en el conjunto de prueba con los valores predichos por el modelo. Idealmente, estos valores deber√≠an estar cerca de una l√≠nea diagonal, que representa la perfecci√≥n (es decir, la predicci√≥n es exactamente igual al valor real). Aqu√≠ tenemos un ejemplo de c√≥mo hacer esto en Python con `matplotlib`:

```python
import matplotlib.pyplot as plt

plt.scatter(Y_test, Y_pred)
plt.xlabel('Valores Reales')
plt.ylabel('Predicciones')
plt.title('Valores Reales vs Predicciones')

# Dibujar la l√≠nea de perfecci√≥n
diagonal = np.linspace(min(Y_test.min(), Y_pred.min()), max(Y_test.max(), Y_pred.max()))
plt.plot(diagonal, diagonal, '-r')
plt.show()
```

El resultado ser√° el siguiente:

![Untitled](./imagenes/Untitled%2032.png)

Este gr√°fico ayudar√° a visualizar c√≥mo de cerca est√°n las predicciones de los valores reales. Si el modelo es perfecto, todos los puntos estar√≠an sobre la l√≠nea roja. Desviaciones de esta l√≠nea representan errores en la predicci√≥n.

## Un buen resumen de lo que vimos

Este [post](https://mlu-explain.github.io/linear-regression/) es un buen resumen de todos los temas que estuvimos viendo de regresi√≥n lineal en esta unidad. 

# Referencias no listadas anteriormente

(1) Introduction to Modern Statistics Mine √áetinkaya-Rundel and Johanna Hardin

[https://www.youtube.com/watch?v=queK1reC-ac](https://www.youtube.com/watch?v=queK1reC-ac)

LINEAR REGRESSION - A Visual Introduction To (Almost) Everything You Should Know: [https://mlu-explain.github.io/linear-regression/](https://mlu-explain.github.io/linear-regression/)